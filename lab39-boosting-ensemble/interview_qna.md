# Interview Q&A — Ensemble Methods (Boosting)

**Q1:** What is boosting?
> Boosting combines multiple weak learners sequentially to form a strong learner.

**Q2:** How does AdaBoost work?
> Each new model focuses more on errors made by previous models, adjusting weights dynamically.

**Q3:** Difference between Bagging and Boosting?
> Bagging reduces variance by parallel training; Boosting reduces bias by sequential learning.

**Q4:** Why compare with Stacking?
> Stacking uses multiple diverse models and a meta-model for final prediction — useful for complex relationships.

**Q5:** When should you prefer Boosting?  
> When your dataset is small or slightly noisy and bias reduction is more critical.
